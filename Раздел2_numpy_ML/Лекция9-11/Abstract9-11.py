# ###  Машинное обучение
#
# # ML - решает следующую задачу
# # Требуется подогнать заданный набор точек с данными под соответсвующуб функцию
# # (  отображение входа на выход), которая улавливает важные сигналы в данных и
# # и игноритует помехи, а затем убедиться, что на новых данных найденная функция
# # работает хорошо
#
# # Обучение с учителем (supervised learning)
# # Обучение без учителя (unsupervised learning)
#
# # ОсУ - моделирует связи между признаками и метками. Такие модели служат для пердсказания
# # меток на основе обучающих данных (маркированных). После построения модели можно использовать ее для присвоения меток
# # новым, ранее неизвестным данным.
# # - задачи классификации (метки - дискретные: два или более)
# # - задачи регрессии (метки/результат; непрерывные величины)
#
# # ОбУ - моделирование признаков без меток. Такие модели служат для выявления структуры немаркированных данных.
# # - задача кластеризации (выделяет отдельные группы данных из общей совокупности)
# # - задача понижения размерности (поиск более сжатого представления данных)
#
# # Существуют методы ЧАСТИЧТНОГО ОБУЧЕНИЯ (semi-supervised learning). Не все данные промаркированы
#
# # Методы обучения с подкреплением (reinforcement learning). Система обучения улучшает свои характеристика на основе
# # взаимодействия (обратной связи) со средой. При этом взаимодействии система получает сигналы (функции наград),
# # которые несут в себе информацию на сколько хорошо или плохо ситсема решила задачу (с точки зрения среды), в которой
# # она работает. Итоговая награда не станет максимальной
#
# import seaborn as sns
#
# iris = sns.load_dataset('iris')
#
# print(iris.head())
# print(type(iris))
#
# print(type(iris.values))
#
# print(iris.values.shape)
#
# print(iris.columns)
#
# print(iris.index)
#
# # Строки - отдельные объекты-образцы (sample)
# # Столбцы - признаки (features) - соответсвуют конкретным наблюдениям
# # Матрица признаков (feature matrix) размер [число образцов * число признаков]
# # Целевой массив, массив меток (targets). Одномерный массив [1 * число образцов]
# # Целевой массив - данные, которые мы хотим предсказать на основе имеющихся данных
# # Зависимые (метка) и независимые переменные (признаки)
#
# # Процесс построения системы мащинного обучения:
#
# # 1. Предварительная обработка
# # - На вход поступают необработанные данные и метки
# # - Происходит выбор признаков, масштабирование признаков
# # - Понижение размерности (выброс ненужных данных)
# # - Выборка образцов
# # - На выходе набор данных: обучающий набор, тестовый набор
#
# # 2. Обучение
# # - Выбор модели
# # - Перекрестная проверка
# # - Метрики эффективности
# # - Оптимизации гиперпараметров. Пареметры, которые получаются не из данных, а являются характеристиками модели.
#
# # 3. Оценка и формирование финальной модели
#
# # 4. Прогнозирование (использование модели)
#
# # библиотека для ML - SciKit-learn
#
# # 1. Выбираем класс модели
# # 2. Устанавливаем гиперпараметры модели
# # 3. Создаем матрицу признаков и целевой массив
# # 4. Обучением модели fit()
# # 5. Применять модель к новым данным
# # - predict() (с учителем)
# # - predict() или transform() (без учителя)
#
#
# # Обучение с учителем: Линейная регрессия
#
# ## Простая линейная регрессия
#
# # y = a1x1+ a2x2 + b
#
# import matplotlib.pyplot as plt
# import numpy as np
#
# # STEP 1
#
# np.random.seed(1)
# x = 10 * np.random.rand(50)
#
# y = 2 * x - 1 + np.random.randn(50)
# plt.scatter(x, y)
#
# # STEP 2
#
# # 1. Выбираем класс модели
# from sklearn.linear_model import LinearRegression
#
# # 2. Устанавливаем гиперпараметры модели
# model = LinearRegression(fit_intercept=False)
#
# # 3. Создаем матрицу признаков и целевой массив
# print(x.shape, y.shape)
#
# X = x[:, np.newaxis]
#
# # 4. Обучением модели fit()
# model.fit(X, y)
#
# print(model.coef_[0])
# print(model.intercept_)
#
# x_ = np.linspace(0, 10, 30)
# y_ = model.coef_[0] * x_ + model.intercept_
#
# plt.plot(x_, y_)
#
# # 5. Применять модель к новым данным
#
# xfit = np.linspace(-10, 10, 5)
# yfit = model.predict(xfit[:, np.newaxis])
#
# plt.scatter(xfit, yfit)
# plt.show()


#######################################################################################

# Линейная регрессия
#
# # Залача: на основе наблюдаемых точек построить прямую, которая отображает связь между двумя или более
# # переменными
# # Регрессия пвтается подогнать функцию к наблюдаемым данным, чтобы спрогнозировать новые данные
# # Линейная регрессия подгоняет данные к прямой линии, пытаемся установить линейную связь между переменными
# # и предсказать новые.
# import random
# import numpy as np
# from sklearn.datasets import make_regression
# from sklearn.linear_model import LinearRegression
# import matplotlib.pyplot as plt
# from numpy.linalg import inv, qr
# # features, target = make_regression(
# #     n_samples=100, n_features=1, n_informative=1, n_targets=1, noise=20, random_state=1
# # )
# #
# # print(features.shape)
# # print(target.shape)
# #
# # model = LinearRegression().fit(features, target)
# #
# # plt.scatter(features, target)
# #
# # x = np.linspace(features.min(), features.max(), 100)
# #
# # ## y = kx + b
# #
# # plt.plot(x, model.coef_[0] * x + model.intercept_, color='red')
# #
# # plt.show()
#
# ## Простая линейная регрессия
#
# # Линейная -> линейная зависимость.
#
# # Плюсы:
# # + прогнозирование на новых данных
# # + анализ взаимного влияния переменных друг на друга
# #
# # - точки обучаемых данных НЕ будут лежать на прямой => область погрещности
# # - НЕ позволяет делать прогнозы вне диапазона имеющихся данных
#
# # Данные, на основании которых разрабатывается модель - это выборка из совокупности, хотелось бы, чтобы
# # эта выборка была репрезентативна
#
# # Остатки\отклонения\ошибки - расстояние между точками данных и ближайшими (по вертикали) точками на прямой.
# # Задача состоит в минимизации остатков -> разрыв между точками и прямой будет минимален.
# # Обучение модели сводится к минимизации функции потерь (Сделать сумму квадратов остатков минимальной)
#
# # Решения: численное (проще и доступнее, решение с погрешностью)
# # и аналитическое (точны, непросто или невозможно получить)
#
# # Аналитическое решение
#
# data = np.array(
#     [
#         [1,5],
#         [2,7],
#         [3,7],
#         [4,10],
#         [5,11],
#         [6,14],
#         [7,17],
#         [8,19],
#         [9,22],
#         [10,28]
#     ]
# )
# x = data[:, 0]
# y = data[:, 1]
#
# n = len(x)
#
# w_1 = ((n * sum(x[i] * y[i] for i in range(n))) - sum(x[i] for i in range(n)) * sum(y[i] for i in range(n))) / ((n * sum(x[i] ** 2 for i in range(n))) - (sum(x[i] for i in range(n)) ** 2))
#
# w_0 = (sum(y[i] for i in range(n))) / n - w_1 * (sum(x[i] for i in range(n))) / n
#
# print(w_1, w_0)
# # w_1 = 2.4, w_0 = 146.0
# # 2.4 146.0
#
#
# # Метод обратных матриц
# # y = w_1 * x + w_0
#
# x_1 = np.vstack([x, np.ones(len(x))]).T
#
# w = inv(x_1.transpose() @ x_1) @ (x_1.transpose() @ y)
#
# print(w)
#
# # Разложение матриц
#
# # X = Q * R -> w = R^-1 * Q.T * y
# Q, R = qr(x_1)
# w = inv(R).dot(Q.transpose()).dot(y)
# print(w)
#
# #
# # # Градиентный спуск - метод оптимизации, где используются производные и итерации
# #
# # # Частная производная по одному из параметров позволяет определить угловой коэффициент и изменение параметра
# # # выполняется в ту сторону, где он максимален/минимален.
# # # Для больших угловых коэффициентов - более широкий "шаг"
# # # Ширина шага определяется как доля от углового коэффициента -> скоростью обучения.
# # # Чем выше скорость, тем быстрее будет работать система, за счет снижения точности.
# # # Чем ниже скорость, тем медленнее и точнее работает система.
# #
# # def f(x):
# #     return (x-3) ** 2 + 4
# #
# # def dx_f(x):
# #     return 2 * x - 6
# #
# #
# # # x = np.linspace(-10, 10, 100)
# # # ax = plt.gca()
# # # ax.xaxis.set_major_locator(plt.MultipleLocator(1))
# # #
# # # # plt.plot(x, f(x))
# # # plt.plot(x, dx_f(x))
# # # plt.grid()
# # # plt.show()
# #
# # L = 0.001
# #
# # iterations = 100_000
# # x = random.randint(0, 5)
# #
# # for i in range(iterations):
# #     d_x = dx_f(x)
# #     x -= L * d_x
# #
# # print(x, f(x))
# # # x = 3, f(x) = 4
#
# ## Как градиентный спуск работает с линейной регрессией?
# ## y = w0 + w1 * x
#
# data = np.array(
#     [
#         [1,5],
#         [2,7],
#         [3,7],
#         [4,10],
#         [5,11],
#         [6,14],
#         [7,17],
#         [8,19],
#         [9,22],
#         [10,28]
#     ]
# )
# x = data[:, 0]
# y = data[:, 1]
#
# n = len(x)
#
# w1 = 0.0
# w0 = 0.0
# L = 0.001
#
# iterations = 100000
# for i in range(iterations):
#     D_w0 = - 2 * sum(y[i] - w0 - w1 * x[i] for i in range(n))
#     D_w1 = - 2 * sum(x[i] * (y[i] - w0 - w1 * x[i]) for i in range(n))
#
#     w1 -= L * D_w1
#     w0 -= L * D_w0
# print(w1, w0)
#
# w1 = np.linspace(-10, 10, 100)
# w0 = np.linspace(-10, 10, 100)
#
# def E(w1, w0, x, y):
#     return sum((y[i] - (w0 + w1 * x[i])) ** 2 for i in range(len(x)))
# W1, W0 = np.meshgrid(w1, w0)
# EW = E(W1, W0, x, y)
#
# fig = plt.figure()
#
# ax = plt.axes(projection='3d')
#
# ax.plot_surface(W1, W0, EW)
#
# w1_fit = 2.4
# w0_fit = 0.8
#
# E_fit = E(w1_fit, w0_fit, x, y)
#
# ax.scatter3D(w1_fit, w0_fit, E_fit, color='red')
#
# plt.show()


##############################################################################
# Переобучение и дисперсия
# # Минимизация квадратов остатков не гарантирует точность Линейной регрессии
# # Цель состоит в не в минимизации суммы квадратов, а в тои чтобы делать правильные предсказания на новых данных
# # Сильно переобученные модели - точно подстраивают регрессию под обучающие данные. (кривая по точкам-признакам)
# # Переобученные модели - чувствительны к выбросам, которые находятся далеко от остальных точек, в прогнозах будет
# # очень высокая дисперсия.
# # Поэтому в модели специально добавляется смещение.
# # Смещение модели означает, что предпочтение отдается определенной схеме (например, прямая линия, пересечение в (0,0)),
# # а не графикам сложной структуры, минимизирующие min(sum(R^2)).
# # Если мы специально добавим смещение, то есть риск недообечения.
# # Балансировка:
# # - минимизация функции потерь -> переобучение
# # - смещение -> недообучение
#
# # Виды регрессий:
# # - Гребневая регрессия (ridge) добавляется смещение в виде некоторого штрафа, которые меняет значение функции, из-за
# # этого идет хуже подгонка
# # - Лассо-регрессия - удаление некоторых переменных (снижение размерности)
#
# # Механически применить линейную регрессию к данным, сделать на основе полученной модели прогноз, и думать, что все в
# # порядке -- НЕЛЬЗЯ
#
# import random
# import numpy as np
# import pandas as pd
# from sklearn.datasets import make_regression
# from sklearn.linear_model import LinearRegression
# from sklearn.model_selection import train_test_split, KFold, cross_val_score
# import matplotlib.pyplot as plt
# from numpy.linalg import inv, qr
#
# data = np.array(
#     [
#         [1, 5],
#         [2, 7],
#         [3, 7],
#         [4, 10],
#         [5, 11],
#         [6, 14],
#         [7, 17],
#         [8, 19],
#         [9, 22],
#         [10, 28]
#     ]
# )
#
# # Градиентный спуск - пакетный градиентный спуск. Для работы используются ВСЕ доступные обучающие данные.
# # На практике используется СТОХАСТИЧЕСКИЙ градиентный спуск, на каждой итерации обучаемся только по одной
# # выборке из данных.
# # - сокращение числа вычислений
# # - вносим смещение => боремся с переобучением
#
# # Мини-пакетный градиентный спуск, на каждой итерации используется несколько выборок
# x = data[:, 0]
# y = data[:, 1]
#
# n = len(x)
#
# w1 = 0.0
# w0 = 0.0
# L = 0.001
#
# # размер выборки
# sample_size = 1
#
# iterations = 100000
#
# for i in range(iterations):
#     idx = np.random.choice(n, sample_size, replace=False)
#     D_w0 = - 2 * sum(y[idx] - w0 - w1 * x[idx])
#     D_w1 = - 2 * sum(x[idx] * (y[idx] - w0 - w1 * x[idx]))
#
#     w1 -= L * D_w1
#     w0 -= L * D_w0
# print(w1, w0)
#
# # Как оценить результат, как сильно промахиваются прогнозы при применении линейной регрессии?
# # Для оценки используется линейный коэффициент корреляции
#
# data_df = pd.DataFrame(data)
# print(data_df.corr(method='pearson'))
# #
# # data_df[1] = data_df[1].values[::-1]
# # print(data_df.corr(method='pearson'))
#
# # Коэффициент корреляции помогает понять есть ли связь между двумя переменными.
#
# # Обучающие и тестовые выборки
# # Основной метод борьбы с переобучением, заключается в том, что набор данных делится на обучающую и тестовую выборки.
#
# # Во всех видах обучения с учителем это встречается
# # Обычная пропорция 2/3 - на обучение, 1/3 на тест. (4/5 к 1/5, 9/10 к 1/10)
#
# X = data_df.values[:, :-1]
# Y = data_df.values[:, -1]
#
# # print(X)
# # print(Y)
#
# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=1 / 3)
# print(X_train, Y_train, X_test, Y_test)
#
# model = LinearRegression()
# model.fit(X_train, Y_train)
#
# r = model.score(X_test, Y_test)   # Коэффициент детерминации
# print(r)
#
# # Коэффициент детерминации r^2 = 1 - (sum((yi - y^i)**2)/(sum((yi - y_i)**2)
# # y_i - среднее для всех точек
# # y^i - предсказанные значения
# # yi - фактические значения
# # Чем ближе r^2 к 1 - тем лучше регрессия работает на тестовых данных, к 0 - хуже.
# #                        0 < r^2 < 1
#
# # Кроссвалидация
# print('----------------')
# kfold = KFold(n_splits=3, random_state=1, shuffle=True)  # 3-х кратная перекрестная валидация
# model = LinearRegression()
# results = cross_val_score(model, X, Y, cv=kfold)
#
# print(results)    # Средне квадратические ошибки
# # Метрики показывают насколько ЕДИНООБРАЗНО ведет себя модель на разных выборках
# print(results.mean(), results.std())
#
# # Возможно использование поэлементной перекрестной валидации ---- когда мало данных
# # Случайная валидация -
#
# # Валидационная выборка - для сравнения различных моделей или конфигураций
#
# # Многомерная линейная регрессия
#
# data_df = pd.read_csv('multiple_independent_variable_linear.csv')
# print(data_df.head())
#
# X = data_df.values[:, :-1]
# Y = data_df.values[:, -1]
#
# model = LinearRegression().fit(X, Y)
# print('------------')
# print(model.coef_, model.intercept_)
#
# x1 = X[:, 0]
# x2 = X[:, 1]
# y = Y
#
# fig = plt.figure()
# ax = plt.axes(projection='3d')
# ax.scatter3D(x1, x2, y)
#
# x1_ = np.linspace(min(x1), max(x1), 100)
# x2_ = np.linspace(min(x2), max(x2), 100)
#
# X1_, X2_ = np.meshgrid(x1_, x2_)
#
# Y_ = model.intercept_ + model.coef_[0] * X1_ + model.coef_[1] * X2_
#
# ax.plot_surface(X1_, X2_, Y_, cmap='Greys', alpha=0.2)
#
# plt.show()